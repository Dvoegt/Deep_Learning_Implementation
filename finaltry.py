#Import Required Libraries
import gym
import random
import numpy as np
import matplotlib.pyplot as plt
from keras import layers, models
import tensorflow as tf

def chooseAction_N(q_values):
    return np.argmax(q_values)

def tensor_state(random_state):
    #convert random state into tensor for nn
    state_array = np.array([random_state])
    state_tensor = tf.convert_to_tensor(state_array, dtype=tf.int32)
    return state_tensor

def nn_output(random_state, model):
    state_tensor = tensor_state(random_state)
    #get Qvalues from given state
    q_values = model.predict([state_tensor], verbose = 0) 
    max_qvalue = np.amax(q_values)
    action = np.argmax(q_values)
    return(q_values, max_qvalue, action) #return the predicted action and q value with it


#Main Method for running the model ---- With Linear Decay for Epsilon 
def runModel(flag, iterations, learning_rate, discount_factor, starting_epsilon, decay_rate):
    #Initialise the Scenario, create Qtable, create standard learning rate, discount factor and epsilon
    env = gym.make('Taxi-v3')
    state_size = env.observation_space.n
    action_size = env.action_space.n
    epsilon = starting_epsilon + decay_rate
    epsilon_minimum = 0.1
    evaluatory_int = 0


    # Create the neural network model
    model = models.Sequential([
    layers.Dense(24, activation='relu', input_dim = 1),
    layers.Dense(24, activation='relu'),
    layers.Dense(env.action_space.n)  # Output layer: Q-value for each action
    ])

    # Compile the model with appropriate optimizer and loss function before training
    model.compile(optimizer='adam', loss='mse')  # Example, suitable for Q-learning


    #Run the model n times

    batch_size = 32  # Number of experiences to collect before updating the model
    experience_buffer = []  # List to store experiences


    rewards_collection = {}  # Track average rewards
    for i in range(iterations):
        if epsilon > epsilon_minimum:
            epsilon = epsilon - decay_rate
    
        random_state = env.reset()  # Get a random initial state
        current_state = random_state[0]
        

        total_reward = 0

        for i in range(1000):

            q_values, max_qvalue, action = nn_output(current_state, model)  #get estimated q_values, the max value and predicted action for that state
            if random.uniform(0,1) < epsilon:
                action = env.action_space.sample() #pick random action at a rate determined by epsilon

            holder = env.step(action)                      #Conduct action, recording the output
            new_state, reward, done, _ = holder[:4]

            #get attributes needed to find target q values
            next_qvalues, next_max_qvalue, next_action = nn_output(new_state, model) 
            target_value = learning_rate * (reward + (discount_factor * next_max_qvalue)) #bellman equation 
        
            #place highest target q_value back into original q_values output generated by neural network
            target_qvalues = np.copy(q_values)
            max_qvalue_index = np.argmax(target_qvalues)
            target_qvalues[0][max_qvalue_index] = target_value

            current_tensor_state = tensor_state(current_state)  #prepare state so its in right format to go into neural network
            model.fit(current_tensor_state, target_qvalues, verbose = 0)

            total_reward += reward  #Update total reward for performance visualisation
            evaluatory_int += reward
            random_state = new_state   
            

            if done:        #When Taxi Env end conditions are met, break the loop and end the iteration
                break

            #if i == 300:
            #    print('action is ', action)

        if (i+1) % 2 == 0:
            rewards_collection.update({i+1: total_reward})
        print('TOATAL REWARD : ', total_reward)

    if flag == True:
        #get x and y
        keys = list(rewards_collection.keys())
        values = list(rewards_collection.values())

        #Create Graph
        graph(keys, values)

        #holder for now
        print("Model Successful")
    else:
        return evaluatory_int/iterations ####Returns Average Reward ----> for evaluating which learning rate and discount factor is best


def graph(x, y):

    # Plotting the line graph
    plt.figure(figsize=(8, 6))
    plt.plot(x, y, marker='o', linestyle='-', color='blue')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('Line Graph of X AND y')
    plt.grid(True)
    plt.show()


runModel(True, 50, learning_rate=0.1, discount_factor=0.1, starting_epsilon=1, decay_rate= 0.01)
